\documentclass[doc,norsk]{apa7}
\usepackage[style=apa]{biblatex}
\usepackage[utf8]{inputenc}
\usepackage[norsk]{babel}


\DeclareLanguageMapping{norsk}{norsk-apa}

\bibliography{references.bib}

\title{Metodologi for replikasjonsprosjekt}
\author{Ole Fredrik Borgundvåg Berg}
\affiliation{NTNU}

\begin{document}

\maketitle

Dette er et replikasjonsprosjekt som er basert på et tilsvarende prosjekt som ble gjort av Ulrich Schimmack, der man prøvde å finne i hvilken grad vitenskaplige funn ved ulike amerikanske universiteter kunne replikeres, inkludert estimert replikasjonsrate på enkeltforskere \parencite{amerikansk-ranking}. Siden faktisk replikasjon er dyrt, så laget heller han med flere en modell som finner estimert replikasjonrate basert på p-verdier i artiklene til forskerene, som er implementert i biblioteket \guillemetleft z-curve\guillemetright\ til R \parencite{z-curve-modell, z-curve-implementasjon}.

\subsection{Finne forskere og artikler}
For å finne artikler og forskere ble Cristin APIet brukt \parencite{cristin-api}. APIet gjør det enkelt både å finne ut hvilke forskere som er tilknyttet hvilke institutt og hvilke artikler som er skrevet av hvilke forskere. Artiklene som er brukt i dette prosjektet er de publikasjonene som har \texttt{category.code = "ARTICLE"} i APIet. Selve nedlastingen av artiklene skjedde også ved programmerisk, men ikke ved hjelp av Cristin. Dette kan føre til at noen av artiklene i Cristin ikke kom med i dette prosjektet pga. manglende tilgang.

\subsection{Ekstrahering av z-verdier}
Siden p-verdier ofte er på formen $p < 0.05$ er det vanskelig å lage et histogram ut fra disse. Derfor brukes kun statistiske tester der man har test-statistikken tilgjengelig. Deretter konverteres den til en z-verdi med tilsvarende p-verdi. Selve ekstraheringen av de statistiske testene ble gjort ved hjelp av verktøyet JATSdecoder og funksjonen \texttt{get.stats()} \parencite{jatsdecoder}. Skal sies at z-curve også har denne funksjonaliteten, men ved egen testing og i følge \textcite{jatsdecoder} sine forsøk så vil JATSdecoder hente ut en større andel av de statistiske testene i artiklene.

\subsection{Modellering av replikasjonsrate og visualisering}
For å finne forventet replikasjonrate og for å lage grafer for visualisering ble de statistiske testene som ble hentet ut ved hjelp av JATSdecoder puttet inn i z-curve. Den både regnet ut forventet replikasjonrate og laget graf over de tilsvarene z-verdiene for de ulike statistikkene i artiklene. For mer informasjon om hvordan z-curve fungerer, se \textcite{z-curve-modell} og \textcite{z-curve-implementasjon}. 

\subsection{Kode}
Koden som ble brukt i dette prosjektet er tilgjengelig på \url{https://github.com/OleFredrik1/Replikasjonsprosjekt}.

\section{Hva betyr de ulike begrepene?}
En hypotese er en påstand om noe og er det man skal teste i en artikkel. Man opererer med en nullhypotese og en alternativ hypotese. Nullhypotesen er den mest konservative og som gjør færrest mulige antagelser. Typisk er nullhypotesen at det ikke er noen sammeheng mellom to eller flere variabler, mens den alternative hypotesen sier at det er en sammenheng.

En p-verdi er sannsynligheten for at man ser noe som er like eller mer ekstremt enn de dataene man faktisk får, gitt at nullhypotesen er sann. OBS: Dette er ikke det samme som sannsynligheten for at nullhypotesen er sann.

Om p-verdien man får er lavere en et forhåndsbestemt signifikansnivå sier man at resultatet er statistisk signifikant og man forkaster nullhypotesen. I psykologi (og en del andre fagfelt) er dette satt til $0.05$. Merk at dersom nullhypotesen er sann er det fortsatt 5\% sjanse for at man får et signifikant resultat.

En z-verdi er en normalfordelt verdi. Vi har at $\Pr[|Z| \leq 1.96] \approx 0.05$, eller med andre ord and en z-verdi med absoluttverdi på 1.96 eller mer vil ha en p-verdi på 0.05 eller mindre.

\section{Hva er p-hacking}
P-hacking er å gjøre en rekke triks for å prøve å få lavere p-verdier. Det finnes mange ulike måter å p-hacke på. En ting er å prøve ulike statistiske tester og så velge de som gir lavest p-verdier. Siden ulike statistiske tester varierer litt i hvordan de funker kan de gi litt ulike p-verdier. En annen ting man kan gjøre er å fjerne observasjoner som ikke passer til hypotesen ved å si at de er outliers. En tredje måte er å endre modellen man bruker helt til man får signifikante resultater. Ved å endre nok kan man ende opp med en signifikant p-verdi til slutt selv om nullhypotesen er sann. Noe annet kan være å se på subgrupper dersom ikke effekten stemmer for hele populasjonen. Kanskje det stemmer bare for kvinner, de mellom 40-50 år eller de med høy inntekt. Hvis du leter nok, kan du nok finne en subgruppe som gir deg lav nok p-verdi. Alt dette er forskningsjuks dersom man ikke justerer for multippel hypotesetesting, men heller bare publiserer de signifikante resultatene og forkaster resten uten å nevne det.

En interessant anekdote er den studien som fant sammenheng mellom fullmåne og selvmord, men bare hos kvinner under 45 år og bare på vinteren \parencite{selvmord-vinter-2}. Denne klarte merkelig nok ikke å replikeres \parencite{selvmord-vinter}.

\section{Hva er publiseringsbias}
Om det er 5\% sjanse for at man forkaster nullhypotesen dersom det stemmer, betyr det da at om man søker etter artikler som tester en hypotese så vil 95\% av artiklene ha nullresultater. Ikke nødvendigvis. Det er en tendens til at artikler som ikke har funnet noe signifikant blir publisert mindre enn artikler som har funnet noe signifikant. Dette kalles publiseringsbias. I verste tilfelle, hvis ingen av nullresultatene publiseres, men kun de 5\% som får signifikante resultater, så vil alle artiklene på område si at hypotesen stemmer selv om den ikke gjør det.

Grunnene til publiseringsbias er flerfoldige. Litt av det er at det gir mer prestisje å publisere signifikante resultater enn nullresultater og om man får nullresultater kan det være fristende å heller legge resultatene i en skuff og starte et nytt forsøk, heller enn å måtte skrive artikkel og  dem. Det kan også være at journaler har mer lyst til å publisere signifikante resultater heller enn nullresultater fordi de anses å være mer spennende. Det kan også være at forskere ikke publiserer nullresultater fordi man tror man har gjort noe feil, spesielt hvis nullresultatet skiller seg ut fra forskningen på området ellers.

Når man lager meta-analyser må man justere for publiseringsbias, for om man ikke gjør det går det veldig feil. En studie fant at i psykologi ville man sett at en median sannsynlighet på 98.9\% for at en meta-analyse ville funnet en effekt om man ikke justerte for publiseringsbias, mens om man justerte for publiseringsbias ville sannsynligheten gå ned til 55.7\% \parencite{publiseringsbias-psykologi}. Altså fra å nesten helt sikkert å finne en effekt, til å ende opp med kron og mynt.

En annen effekt er at forskning som ikke kan replikeres siteres oftere, muligens fordi de har mer spennende funn, men som dessverre viser seg å være for godt til å være sant \parencite{falsk-sitert-mer}.

\subsection{Hvorfor det meste av forskning ikke stemmer}
\textcite{forskning-fake} sin artikkel har det litt alarmerende navnet \guillemetleft Why Most Published Research Findings Are False\guillemetright. En faktor som ikke er nevt til nå er hva som er apriori sannsynlighet for at en hypotese stemmer? Se for deg at du har et lykkehjul med variabler, som for kjønn, alder, inntekt, utdanning, introversjon, reisevaner, klesvalg, bruk av et bestemt legemiddel, depressive symptomer, vel alt du kan tenke deg. Deretter spinner man dette hjulet to ganger for å finne to variabler A og B. Hva er da sannsynligheten for at det er noen sammenheng mellom de to variablene? \textcite{forskning-fake} argumenterer med at det i mange tilfeller er ganske lav sjanse.
Om $T$ er hendelsen at hypotesen stemmer, $F$ er hendelsen at hypotesen ikke stemmer, $+$ er hendelsen at man får et signifikant resultat og $-$ er hendelsen at man ikke får et signifikant resultat, så kan vi finne $\Pr[T|+]$, altså sannsynligheten for at en hypotese stemmer, ved å bruke Bayes' teorem:
\begin{equation}
\Pr[T|+] = \frac{\Pr[+|T]\Pr[T]}{\Pr[+]}
\end{equation}
Hvor $\Pr[+|T]$ er sannsynligheten for at man får et signifikant resultat gitt at hypotesen stemmer og kalles ofte styrken til testen. Man setter typisk opp forsøket slik at denne verdien er antatt å være $0.8$. $\Pr[+]$ kan utvides til $\Pr[+|T]\Pr[T] + \Pr[+|F]\Pr[+]$, hvor $\Pr[+|F]$ er signifikansnivået vårt $0.05$. Om man setter $\Pr[T]$ til å være $0.1$ (altå én av ti hypoter stemmer), vil man få en sannsynlighet på $\frac{0.8\times 0.1}{0.8\times 0.1 + 0.05\times 0.9} = 0.64$ for at en hypotese stemmer gitt at man får et signifikant resultat. Dette vil tilsi at rundt to av tre signifikante resultater vil tilsi at hypotesen stemmer. Hvis man legger til publiseringsbias i likingen blir sannsynligheten enda lavere. \textcite{forskning-fake} antar at denne sannsynligheten er under $50\%$ i mange tilfeller, derav tittelen på artikkelen.

Skal nevnes at \textcite{forskning-fake} har fått en del kritikk av andre forskere, blant annet skaperen av z-curve, som mener at \citeauthor{forskning-fake} sin påstand om at det meste av forskning er feil ikke stemmer med dataene man har \parencite{forskning-fake-kritikk}.

\printbibliography

\end{document}